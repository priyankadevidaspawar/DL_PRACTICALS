{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUNn6NYKuW9ytLkNiGvBtB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyankadevidaspawar/DL_PRACTICALS/blob/main/Useful_NLP_Libraries_NlTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDFJPddhJDwt",
        "outputId": "91f77776-cba8-4940-f3a0-97d331f48c1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK using pip\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import NLTK\n",
        "import nltk\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0iuK7BuJI1d",
        "outputId": "c3aa3471-e9b9-4fcc-cc8b-84bb72e7c82a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLTK is a powerful tool for natural language processing. It can tokenize sentences and words. NLTK includes various NLP libraries for text analysis.\""
      ],
      "metadata": {
        "id": "WTHVulApJP8y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "vYS7ld1XJjgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "print(\"Words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8sr8pzjJfXv",
        "outputId": "07927b97-5efd-47ea-8127-882ad8034d7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'natural', 'language', 'processing', '.', 'It', 'can', 'tokenize', 'sentences', 'and', 'words', '.', 'NLTK', 'includes', 'various', 'NLP', 'libraries', 'for', 'text', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-of-speech tagging"
      ],
      "metadata": {
        "id": "7WEiIlZlP_FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "pos_tags = pos_tag(words)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c6KeEu3QAS0",
        "outputId": "a3e50b13-5675-4606-e679-54a3e8e856b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.'), ('It', 'PRP'), ('can', 'MD'), ('tokenize', 'VB'), ('sentences', 'NNS'), ('and', 'CC'), ('words', 'NNS'), ('.', '.'), ('NLTK', 'NNP'), ('includes', 'VBZ'), ('various', 'JJ'), ('NLP', 'NNP'), ('libraries', 'NNS'), ('for', 'IN'), ('text', 'JJ'), ('analysis', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 'I' is tagged as a personal pronoun ('PRP').\n",
        "####'love' is tagged as a verb ('VBP').\n",
        "##'natural' is tagged as an adjective ('JJ').\n",
        "##'language' and 'processing' are both tagged as nouns ('NN').\n",
        "##in=prepostion\n"
      ],
      "metadata": {
        "id": "LHo_h7ZWQFgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "BF5NmyXQRry2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMcRSga5RnaE",
        "outputId": "a9934ab0-4f37-47c6-8fd7-096f02651efa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['nltk', 'is', 'a', 'power', 'tool', 'for', 'natur', 'languag', 'process', '.', 'it', 'can', 'token', 'sentenc', 'and', 'word', '.', 'nltk', 'includ', 'variou', 'nlp', 'librari', 'for', 'text', 'analysi', '.']\n",
            "Lemmatized Words: ['NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'natural', 'language', 'processing', '.', 'It', 'can', 'tokenize', 'sentence', 'and', 'word', '.', 'NLTK', 'includes', 'various', 'NLP', 'library', 'for', 'text', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop words removal"
      ],
      "metadata": {
        "id": "a5-W5kReSNBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Filtered Words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxeR-UizRneL",
        "outputId": "ac35589f-7f79-44d5-bb0e-4453e18d1067"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['NLTK', 'powerful', 'tool', 'natural', 'language', 'processing', '.', 'tokenize', 'sentences', 'words', '.', 'NLTK', 'includes', 'various', 'NLP', 'libraries', 'text', 'analysis', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency Distribution"
      ],
      "metadata": {
        "id": "jXXFuOq4SrCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "freq_dist = FreqDist(words)\n",
        "print(\"Frequency Distribution:\", freq_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYdkli0HRnhx",
        "outputId": "ff01a579-4971-4f25-d751-ef28e8697a1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency Distribution: <FreqDist with 22 samples and 26 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concordance and Similarity"
      ],
      "metadata": {
        "id": "hmCT3fjLS_jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.text import Text\n",
        "\n",
        "text_object = Text(words)\n",
        "concordance_result = text_object.concordance(\"NLTK\")\n",
        "similar_words = text_object.similar(\"tool\")\n",
        "print(\"Concordance Result:\", concordance_result)\n",
        "print(\"Similar Words:\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M20AITMNS0jY",
        "outputId": "667b313d-c7e1-4ac0-f60e-e91ab4710600"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 2 of 2 matches:\n",
            " NLTK is a powerful tool for natural langu\n",
            "t can tokenize sentences and words . NLTK includes various NLP libraries for t\n",
            "\n",
            "Concordance Result: None\n",
            "Similar Words: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "BwuW8UEKTWOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_score = sia.polarity_scores(text)\n",
        "print(\"Sentiment Analysis Score:\", sentiment_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XcMte-KTHDo",
        "outputId": "b15c52cb-4176-4a2e-8f0a-98d8fdfb529c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Score: {'neg': 0.0, 'neu': 0.745, 'pos': 0.255, 'compound': 0.6705}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "IuH5ZfvzUS4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags_for_ner = pos_tag(tokens)\n",
        "ner_result = ne_chunk(pos_tags_for_ner)\n",
        "print(\"NER Result:\", ner_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqYdVqtkTaOA",
        "outputId": "501250ee-1f44-4b51-8541-e7fce05355c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER Result: (S\n",
            "  (ORGANIZATION NLTK/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  powerful/JJ\n",
            "  tool/NN\n",
            "  for/IN\n",
            "  natural/JJ\n",
            "  language/NN\n",
            "  processing/NN\n",
            "  ./.\n",
            "  It/PRP\n",
            "  can/MD\n",
            "  tokenize/VB\n",
            "  sentences/NNS\n",
            "  and/CC\n",
            "  words/NNS\n",
            "  ./.\n",
            "  (ORGANIZATION NLTK/NNP)\n",
            "  includes/VBZ\n",
            "  various/JJ\n",
            "  (ORGANIZATION NLP/NNP)\n",
            "  libraries/NNS\n",
            "  for/IN\n",
            "  text/JJ\n",
            "  analysis/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XyKYgdv-UYpC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}